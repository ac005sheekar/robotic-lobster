<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sheekar Banerjee</title>
  
  <meta name="author" content="Sheekar Banerjee">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="sheekar2.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <h1><name>Sheekar Banerjee</name></h1>
              </p>
              <p>I am a prospective graduate student. My field of research interest is related to Robotics, Computer Vision and Graphics and Medical Image. Currently I am working as a Lead AI Engineer in a South Korean Multi-National called <a href="https://www.vinacts.com/">Vinacts</a>, where I lead projects related to AI Digital Human, Computer Vision and Robotics. Previously, I have also worked for multiple Multi-National companies based in United Kingdom, United States and Croatia as an AI researcher. 
              </p>
              <p>
                Prior to this, I completed my dual-degree (B.Tech + M.Tech) from IIT Kharagpur where I worked as an undergraduate researcher at the Computer Vision and Intelligence Research Lab under the supervision of <a href="https://cse.iitkgp.ac.in/~adas/">Prof. Abir Das</a>.
              </p>
              <p>In the summer of 2021, I was fortunate enough to get an opportunity to work under the guidance of <a href="https://www.bu.edu/cs/profiles/saenko/">Prof. Kate Saenko</a> (<a href="https://www.bu.edu/cs/">Boston University</a>) and <a href="https://people.eecs.berkeley.edu/~trevor/">Prof. Trevor Darrell</a> (<a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>), as a research intern for the DARPA LwLL Project.
              </p>
              <p style="text-align:center">
                <a href="mailto:sheekar.banerjee@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/Aadarsh-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/Aadarsh-Bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=TpLbmAcAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="data/Resume_Aadarsh.pdf">Resume</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/SahooAadarsh">Twitter</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/sheekar-banerjee-6b189b12a/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/ac005sheekar">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:40%">
              <a href="sheekar2.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="sheekar2.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr> <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                  <li> [Jan'23] Paper on Anytime Domain Adaptation accepted at <a href="https://iclr.cc/">ICLR 2023</a>.<br>
                  <li> [Jan'23] Select, Label, and Mix (SLM) received the <font color="red">Best Paper Honorable Mention Award at <a href="https://wacv2023.thecvf.com/">WACV 2023</a>!</font>.<br>
                  <li> [Oct'22] Paper on Partial Domain Adaptation accepted at <a href="https://wacv2023.thecvf.com/">WACV 2023</a>.<br>
                  <li> [Jul'22] Started working as a student researcher at the <a href="https://mitibmwatsonailab.mit.edu/">MIT-IBM Watson AI Lab</a> at Cambridge, MA.
                  <li> [Oct'21] Extended Abstract on Partial Domain Adaptation accepted at <a href="https://sites.google.com/view/distshift2021/home">NeurIPS DistShift Workshop 2021</a>.<br>
                  <li> [Sept'21] Paper on Domain Adaptation in Action Recognition accepted at <a href="https://neurips.cc/">NeurIPS 2021</a>.<br>
                  <li> [Jun'21] Volunteer at the workshop on <a href="https://sites.google.com/view/cvpr2021-dnetcv/">Dynamic Neural Networks Meets Computer Vision (DNetCV)</a> at <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.<br>
                  <li> [May'21] Started my internship at UC Berkeley and Boston University.<br>
                  <li> [Aug'20] One paper accepted at the <a href="https://eccv2020.eu/">ECCV 2020</a> Workshop on <a href="https://sites.google.com/view/ipcv2020/call-for-papers">Imbalance Problems in Computer Vision (IPCV)</a>.<br>
                  <li> [May'19] Joined CVIR, IIT Kharagpur as an undergraduate researcher in Computer Vision.<br>
                  <li> [Jul'18] Got Computer Science and Engineering as my major. (less than 1% acceptance)<br>
                  <li> [Jul'17] Got accepted into <a href="http://www.iitkgp.ac.in/">IIT Kharagpur</a> for my undergraduate studies through the <a href="https://en.wikipedia.org/wiki/Joint_Entrance_Examination_%E2%80%93_Advanced">JEE Advanced 2017</a>.<br>
              </ul>
            </td> </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests lie in understanding the <i>principles of learning</i> from multiple modalities and exploring how knowledge from one modality can be transferred to applications in others, with a goal to design embodied multimodal agents benefiting humanity. Obtaining answers to questions like - "Do toddlers use the same principles in learning new languages as they do in learning how to walk?" -  should be fun!
                <!-- My research interests lie broadly in computer vision and artificial intelligence. My current focus majorly is to explore and conduct fundamental computer vision research with limited supervision, with a goal to conduct research and design products benefiting humanity. I am excited to be part of this fast-evolving and fascinating field, and I hope to contribute to its growth. -->
                <!-- and specifically in unsupervised learning and semi/self-supervised learning.  -->
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <!-- Papers list -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2022_AnyDA.png" width="160" height="100">
          </td>
          <td width="75%" valign="middle">
            <a href="data/AnyDA_2022.pdf">
              <papertitle>AnyDA: Anytime Domain Adaptation</papertitle>
            </a>
            <br>
              Omprakash Chakraborty, <strong>Aadarsh Sahoo</strong>, Rameswar Panda, Abir Das<br>
              <em>11th International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023.<br>
            <a href="https://cvir.github.io/projects/anyda.html">project page</a> /
            <a href="https://cvir.github.io/projects/anyda.html">code</a>
            <p> We introduce a novel approach for anytime domain adaptation by considering domain alignment with switchable depth, width and input resolutions to achieve accuracy-efficiency trade-offs in the target domain for different resource constraints.</p>
          </td>
        </tr> </tbody></table>
          


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/NeurIPS_2021_CoMix.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2021/file/c47e93742387750baba2e238558fa12d-Paper.pdf">
                <papertitle>Contrast and Mix: Temporal Contrastive Video Domain Adaptation with Background Mixing</papertitle>
              </a>
              <br>
                <strong>Aadarsh Sahoo</strong>, Rutav Shah, Rameswar Panda, Kate Saenko, Abir Das<br>
              <em>35th Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2021.<br>
              <a href="https://cvir.github.io/projects/comix.html">project page</a> /
              <a href="https://cvir.github.io/projects/images/comix_neurips2021_poster.png">poster</a> /
              <a href="https://slideslive.com/38967894/contrast-and-mix-temporal-contrastive-video-domain-adaptation-with-background-mixing">video presentation</a> /
              <a href="https://docs.google.com/presentation/d/1UBvqIRdl7DkmZN7_3lumpMg5kVyjcOl6B507pw4Ul98/edit#slide=id.gf354b2a2b3_0_351">slides</a> /
              <a href="https://github.com/CVIR/CoMix">code</a>
              <p> We introduce a novel temporal contrastive learning approach for unsupervised video domain adaptation, which is achieved by jointly leveraging video speed, background mixing, and target pseudo-labels.</p>
            </td>
          </tr> </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/arXiv_2020_SLM.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Sahoo_Select_Label_and_Mix_Learning_Discriminative_Invariant_Feature_Representations_for_WACV_2023_paper.pdf">
                <papertitle>Select, Label, and Mix: Learning Discriminative Invariant Feature Representations for Partial Domain Adaptation</papertitle>
              </a>
              <br>
                <strong>Aadarsh Sahoo</strong>, Rameswar Panda, Rogerio Feris, Kate Saenko, Abir Das<br>
              <em>NeurIPS DistShift Workshop (<strong>NeurIPS-W</strong>)</em>, 2021.<br>
              <em>Winter Conference on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2023.<br>
              <em><font color="red"><strong>(Best Paper Honorable Mention)</strong></font>.</em><br>
                <a href="https://cvir.github.io/projects/slm.html">project page</a> /
                <a href="https://video.vast.uccs.edu/WACV23/1177-wacv-post.pdf">poster</a> /
                <a href="https://video.vast.uccs.edu/WACV23/1177_wacv.mp4">video presentation</a> /
                <a href="https://docs.google.com/presentation/d/1JlORqqL4LlOOqaAYqriaJ5g5UAIumURyrGF0BAj0gQY/edit?usp=sharing">slides</a> /
                <a href="https://github.com/CVIR/SLM">code</a>
              <p> We develop a novel 'Select, Label, and Mix' (SLM) framework that aims to learn discriminative invariant feature representations for partial domain adaptation.</p>
            </td>
          </tr> </tbody></table>




          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ECCV_2020_W_Imbalance.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="data/Imbalance_ECCVW_2020.pdf">
                <papertitle>Mitigating Dataset Imbalance via Joint Generation and Classification</papertitle>
              </a>
              <br>
                <strong>Aadarsh Sahoo</strong>, Ankit Singh, Rameswar Panda, Rogerio Feris, Abir Das<br>
              <em>ECCV Workshop on Imbalance Problems in Computer Vision (<strong>ECCV-W</strong>)</em>, 2020 <em><font color="red"><strong>(Oral)</strong></font>.</em><br>
              <a href="https://cvir.github.io/projects/imbalance.html">project page</a> /
              <!-- <a href="https://arxiv.org/abs/2008.05524">arXiv</a> / -->
              <!-- <a href="https://youtu.be/EpH175PY1A0">video</a> / -->
              <!-- <a href="https://github.com/AadSah/ImbalanceCycleGAN">code</a> -->
              <a href="https://cvir.github.io/projects/imbalance.html">code</a> /
              <a href="https://www.youtube.com/watch?v=VLqkW6IAFhE&t=4779s">live talk</a>
              <p> We introduce a joint dataset repairment strategy by combining classifier with a GAN that makes up for the deficit of training examples from the minority class by producing additional examples.</p>
            </td>
          </tr> </tbody></table>

          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications (Under Review)</heading>
            </td>
          </tr>
        </tbody></table>


          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/2022_CPT.png" width="160" height="100">
            </td>
            <td width="75%" valign="middle">
              <a href="data/CPT_2022.pdf">
                <papertitle>Frustratingly Simple Contrastive Prompt Tuning for Vision-Language Models</papertitle>
              </a>
              <br>
                <strong>Aadarsh Sahoo</strong>, Anshuman Senapati, Abir Das, Yoon Kim, Rogerio Feris, Rameswar Panda<br>
              <em>In Submission at (<strong>CVPR</strong>)</em>, 2023.<br>
              <a href="https://coming.soon/projects/cpt.html">project page</a> /
              <a href="https://coming.soon/projects/cpt.html">code</a>
              <p> We introduce a frustratingly simple prompt tuning framework using instance contrastive and relational contrastive loss for improving generalization of vision-language models.</p>
            </td>
          </tr> </tbody></table> -->


      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Ongoing Projects</heading>
          </td>
        </tr>
      </tbody></table>

      <!-- Papers list -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="nlt_stop()" onmouseover="nlt_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <img src="images/2022_GFM.png" width="160" height="100">
          </td>
          <td width="75%" valign="middle">
            <a href="data/GFM_2022.pdf">
              <papertitle>Learning Grounded Foundation Models</papertitle>
            </a>
            <br>
              Advised by: Prof. Yoon Kim, Dr. Rameswar Panda, Dr. Rogerio Feris <br>
            <p> Working on probing existing foundation models (LLMs, VLMs, etc.) for various conceptual knowledge (like color, shape, etc.) and studying entanglement between them (e.g. is ‘rose’ implicitly entangled with ‘red’ ?). We aim to analyze the effectiveness of existing pre-training paradigms in learning grounded knowledge. Further, we will be training novel grounded foundation models using data from modalities like language, vision, and embodied interactions with an explicit focus on learning grounded concepts like intuitive physics, and common sense reasoning.</p>
          </td>
        </tr> </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr> <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Services</heading>
            <ul>
                <li> <strong>Reviewer</strong>: CVPR, NeurIPS, ICLR, ICCV, WACV, TPAMI.
                <li> <strong>Student Volunteer and Reviewer</strong>: <a href="https://sites.google.com/view/cvpr2022-dnetcv/">Dynamic Neural Networks Meets Computer Vision (DNetCV)</a> at <a href="http://cvpr2022.thecvf.com/">CVPR 2022</a>.
                <li> <strong>Student Volunteer and Reviewer</strong>: <a href="https://sites.google.com/view/cvpr2021-dnetcv/">Dynamic Neural Networks Meets Computer Vision (DNetCV)</a> at <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.
            </ul>
          </td> </tr>
      </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Webpage template courtesy: <a href="https://jonbarron.info/">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
